<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?">

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Benchmark, Multimodal Learning, Vision and Language Dataset, Vision Language Model, LLM, VLMm Image Generation, Text-to-Image Generation, T2I, Text to image generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Commonsense-T2I</title>
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
  w[l] = w[l] || []; w[l].push({
    'gtm.start':
      new Date().getTime(), event: 'gtm.js'
  }); var f = d.getElementsByTagName(s)[0],
    j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
      'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
})(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
<!-- End Google Tag Manager -->

<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet">

<link rel="stylesheet" href="./static/css/bulma.min.css">
<link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
<link rel="stylesheet" href="./static/css/bulma-slider.min.css">
<link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
<link rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<link rel="stylesheet" href="./static/css/index.css">
<link rel="icon" href="./static/images/icon-removebg-preview.png">

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script defer src="./static/js/fontawesome.all.min.js"></script>
<script src="./static/js/bulma-carousel.min.js"></script>
<script src="./static/js/bulma-slider.min.js"></script>
<script src="./static/js/index.js"></script>
</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<nav class="navbar" role="navigation" aria-label="main navigation">
<div class="navbar-brand">
  <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true"></span>
    <span aria-hidden="true"></span>
    <span aria-hidden="true"></span>
  </a>
</div>
<div class="navbar-menu">
  <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
    <a class="navbar-item" href="https://zeyofu.github.io/">
    <span class="icon">
        <i class="fas fa-home"></i>
    </span>
    </a>

    <div class="navbar-item has-dropdown is-hoverable">
      <a class="navbar-link">
        More Research
      </a>
      <div class="navbar-dropdown">
        <a class="navbar-item" href="https://zeyofu.github.io/blink/">
          BLINK
        </a>
        <a class="navbar-item" href="https://visualsketchpad.github.io/">
          Visual Sketchpad
        </a>
        <a class="navbar-item" href="https://muirbench.github.io/">
          MuirBench
        </a>
      </div>
    </div>
  </div>

</div>
</nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="static/images/icon.png" width="70" />Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense? </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                  <a href="https://zeyofu.github.io/" target="_blank"><font color="#B082C9"><b>Xingyu Fu</b></font></a>,</span>
                  <span class="author-block">
                    <a href="https://riddlehe.github.io/" target="_blank" style="color:#B082C9"><font color="#B082C9"><b>Muyu He</b></font></a>,</span>
                    <span class="author-block">
                    <a href="https://yujielu10.github.io/" target="_blank"><font color="#5DADE2"><b>Yujie Lu</b></font></a>,</span>
                    <span class="author-block">
                      <a href="https://sites.cs.ucsb.edu/~william/" target="_blank"><font color="#5DADE2"><b>William Yang Wang</b></font></a>,</span>
                      <span class="author-block">
                        <a href="https://www.cis.upenn.edu/~danroth/" target="_blank"><font color="#B082C9"><b>Dan Roth</b></font></a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <font color="#B082C9">&#x25BA;</font>University of Pensylvania&emsp;
                      <font color="#5DADE2">&#x25BA;</font>University of California, Santa Barbara&emsp;
                      <!-- Equal Contribution -->
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2406.07546" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/CommonsenseT2I/CommonsensenT2I" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zeyofu/Commonsense-T2I" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://twitter.com/XingyuFu2/status/1801369092944969736" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
                </span>
                
    
                <span class="link-block">
                  <a href="visualization_dalle3.html" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-images"></i>
                  </span>
                  <span>Visualization DALL-E 3</span>
                </a>
              </span>


                <span class="link-block">
                  <a href="visualization_dalle3_no_revision.html" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-images"></i>
                  </span>
                  <span>Visualization DALL-E 3 w/o prompt revision</span>
                </a>
              </span>
    
                <span class="link-block">
                  <a href="visualization_sd_21.html" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-images"></i>
                  </span>
                  <span>Visualization SD 2.1</span>
                </a>
              </span>

                <span class="link-block">
                  <a href="visualization_sd_xl.html" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-images"></i>
                  </span>
                  <span>Visualization SD XL</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="visualization_LCMs.html" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fa fa-images"></i>
                </span>
                <span>Visualization LCMs</span>
              </a>
            </span>


            <span class="link-block">
              <a href="visualization_playground25.html" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fa fa-images"></i>
              </span>
              <span>Visualization Playground2.5</span>
            </a>
          </span>


            <span class="link-block">
              <a href="visualization_openjourneyv4.html" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fa fa-images"></i>
              </span>
              <span>Visualization OpenJourney v4</span>
            </a>
          </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <h2 class="title is-3 has-text-centered">What is Commonsense-T2I?</h2> -->
      <!-- <h2 class="subtitle has-text-justified">
        An example prompt in <span style="font-weight:bold;"> Commonsense-T2I </span> and failure cases from DALL-E 3 Betker
et al. (2023), Stable Diffusion XL (Rombach et al., 2022), Openjourney v4, and Playground v2.5 Li et al. (2024). The expected output for the prompt is “The lightbulb is unlit”. </h2> -->
      <img src="static/images/teaser.png" height="100%"/>
      <!-- <h2 class="subtitle has-text-centered">Example tasks in <span style="font-weight:bold;">BLINK</span>.</h2> -->
      <!-- <h2 class="hero-body has-text-centered">
        <br>
        An example prompt in Commonsense-T2I and failure cases from DALL-E 3 (Betker et al., 2023), Stable Diffusion XL (Rombach et al., 2022), Openjourney v4, and Playground v2.5 (Li et al., 2024). The expected output for the prompt is “The lightbulb is unlit”. 
      </h2> -->
      <h2 class="subtitle has-text-justified">
        An example prompt in Commonsense-T2I and failure cases from DALL-E 3 (Betker et al., 2023), Stable Diffusion XL (Rombach et al., 2022), Openjourney v4, and Playground v2.5 (Li et al., 2024). The expected output for the prompt is “The lightbulb is unlit”. </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- BLINK Comparison -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">What is <b>Commonsense-T2I</b>?</h2>
        <h2 class="content has-text-justified">
          We present <b>a novel task and benchmark </b>for evaluating the ability of text-to-image(T2I) generation models to produce images that fit commonsense in real life, which we call <b>Commonsense-T2I</b>.
          Commonsense-T2I presents an <b>adversarial challenge</b>, providing pairwise text prompts along with expected outputs. <br>
          <ul>
            <!-- <li>Given two <b>adversarial</b> text prompts containing an identical set of action words with minor differences, such as <mark>"<i>a lightbulb without electricity</i>" v.s. "<i>a lightbulb with electricity</i>"</mark>, we evaluate whether T2I models can conduct visual-commonsense reasoning, eg. produce images that fit <mark>"<i>The lightbulb is unlit</i>" v.s. "<i>The lightbulb is lit</i>"</mark> correspondingly. -->
            <li>Given <b>two adversarial</b> text prompts containing an identical set of action words with minor differences, such as <b>"<i>a lightbulb without electricity</i>"</b> v.s. <b>"<i>a lightbulb with electricity</i>"</b>, we evaluate whether T2I models can conduct visual-commonsense reasoning, eg. produce images that fit <b>"<i>The lightbulb is unlit</i>"</b> v.s. <b>"<i>The lightbulb is lit</i>"</b> correspondingly.
            </li> <br>
            <li>The dataset is carefully <b>hand-curated by experts</b> and annotated with fine-grained labels, such as commonsense type and likelihood of the expected outputs, to assist analyzing model behavior. We benchmark a variety of state-of-the-art (sota) T2I models and surprisingly find that, there is still a large gap between image synthesis and real life photos--even the <b>DALL-E 3 model</b> could only achieve <b>48.92%</b> on <b>Commonsense-T2I</b>, and the <b>Stable Diffusion XL model</b> only achieves <b>24.92%</b> accuracy.</li> <br>
            <li>Our experiments show that GPT-enriched prompts <b>cannot</b> solve this challenge, and we include a detailed analysis about possible reasons for such deficiency.</li>
        </ul>
        </h2>
        <img src="static/images/overall.png" height="100%"/>
        <h2 class="content has-text-centered">
          One example from <b>Commonsense-T2I</b>, where P1, P2 are pairwise prompts; D1, D2 are descriptions for expected output images.
        </h2>
        <h2 class="content has-text-justified">
        
          <ul>
            <li>We aim for <b>Commonsense-T2I</b> to serve as a high-quality evaluation benchmark for T2I commonsense checking, fostering advancements in real life image generation.</li>
          </ul>
          </h2>
      </div>
    </div>
  </div>
</section>

<!-- Paper Qualitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Evaluation Pipeline</h2>
        <img src="static/images/eval_pipeline.png" width="100%"/>
        <!-- <h2 class="content has-text-centered">
          Qualitative results on <b>BLINK</b>. 
        </h2> -->
        <h2 class="content has-text-justified">
          The evaluation pipeline for <b>Commonsense-T2I</b>. P1, P2 (text prompts) and D1, D2 (descriptions for expected output) are provided by <b>Commonsense-T2I</b>, while I1, I2 are generated images. As shown in the figure, a data example is correct only if both of the pairwise prompts are generated correctly.
        </h2>
        
      </div>
    </div>
  </div>
</section>

<!-- Paper Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Qualitative Results</h2>
        <img src="static/images/dalle_revision.png" width="100%"/>
        <!-- <h2 class="content has-text-centered">
          Qualitative results on <b>BLINK</b>. 
        </h2> -->
        <h2 class="content has-text-justified">
          We show error cases of DALL-E 3, and DALL-E 3 w/o revision, which turns off the GPT-revision function on prompts. Input prompts and expected outputs are in the green box. DALL-E 3 images are generated with the revised prompts returned by DALL-E 3 as default, and DALL-E 3 w/o revision images are generated with the original prompt. The highlighted sentences are (partially) correct expected output descriptions in revised prompts, but not illustrated in the output images.
        </h2>
        
      </div>
    </div>
  </div>
</section>


<!-- Paper Quantitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Quantitative Results</h2>
        <img src="static/images/result.png" height="90%"/>
        <h2 class="content has-text-justified">
          Main results on the Commonsense-T2I challenge set. The columns row shows the T2I models that we evaluate on, and the first row shows the evaluator choices. The best performance model under each evaluator is DALL-E 3.
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- Paper Analysis -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Experiment Analysis</h2> <br>
    </div>
    <div class="columns is-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-5">
        Are T2I models limited by text embedding?
        <h2 class="content has-text-justified">
          Since all the Stable Diffusion (based) T2I models score under 35% accuracy on Commonsense-T2I, we investigate the <b>possible reason
behind this phenomena</b>: these models might be biased by the text embedding of the prompts. The motivation is follows: if the embeddings of P1 and P2, which are inputs to the T2I models, are very similar, then they could lead the T2I models to generate similar images for P1 and
P2, while the expected outputs should different. We deploy the CLIP (Radford et al., 2021) (ViT/L14) encoder, which is the default text encoder for Stable Diffusion (based) models, to encode the pairwise prompts P1 and P2 in Commonsense-T2I. We compare the similarity
between CLIP embedding of P1 and P2 against performance score as in Figure 5. Notice that we adopt min-max normalization to project the embedding similarity values into [0,1].
        </h2>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/clip_similarity_performance.png" width="60%"/> </div>
        <h2 class="content has-text-centered">
          When the CLIP embedding similarity of prompts P1, P2 goes up, human evaluated performance scores go down. It suggests that
          T2I models perform badly when their text encoders fail to differentiate between P1 and P2.
        </h2>
        <br>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel examples in BLINK-->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">More Examples</h2> <br></div>
      <div class="columns is-centered">
        <div class="column is-five-sixths">
      <h2 class="content has-text-justified">
        We show random selected error examples for models tested in <b>Commonsense-T2I</b>, find more in the visualizations!
      </h2>
    </h2>
    <div class="columns is-centered has-text-centered">
    <img src="static/images/sdxl_errors.png" width="100%"/> </div>
    <h2 class="content has-text-centered">
      Error cases of Stable Diffusion XL andPlayground v2.5. The prompt and expected output description are provided in green box for each example.
    </h2>
    </div></div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{fu2024commonsenseT2I,
          title = {Commonsense-T2I Challenge: Can Text-to-Image Generation Models Understand Commonsense?},
          author = {Xingyu Fu and Muyu He and Yujie Lu and William Yang Wang and Dan Roth},
          journal={arXiv preprint arXiv:2406.07546},
          year = {2024},
          }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
